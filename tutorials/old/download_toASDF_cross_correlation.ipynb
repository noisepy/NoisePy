{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data downloading into ASDF and computing cross correlation functions \n",
    "\n",
    "In this notebook, we show the basic steps of NoisePy for 1) downloading seismic data using [ObsPy](https://github.com/obspy/obspy/wiki) functions (`get_station` and `get_waveform`) and saving them into ASDF file, and 2) computing cross correlation functions. As you will find through the practise, apart from the data downloading and data loading processes (sections 0-3), the rest of the notebook is essentionally the same to the one shown in the other notebook of `cross_correlation_from_sac`. If you would like to use the NoisePy to deal with SAC/mSEED files at your local disk or any other format that can be ready by ObsPy, we recommend you look at the other notebook first. \n",
    "\n",
    "The steps to compuate cross correlation functions as illustrated in this notebook are:  \n",
    "* Download Z-component data from 2 TA stations, pre-process the data (remove gaps, downsample etc) and save them into one ASDF file\n",
    "* Read the ASDF file with the first station to be source and the other as the receiver\n",
    "* Break the continous data into small segments with overlaps\n",
    "* Perform Fourier Transform to convert signals into frequency-domain\n",
    "* Calculate cross correlation functions between the small time segments and choose to stack (substack) the cross correlation function of each segment and return to time domain\n",
    "* Save the correlation function into an ASDF file\n",
    "\n",
    "More details on the descriptions of data processing, parameters for different cross correlation method and performance of NoisePy can be found in the online [documentations](https://noise-python.readthedocs.io/en/latest/) and the manuscript.\n",
    "\n",
    "`Jiang, C. and Denolle, M. 2020. NoisePy: a new high-performance python tool for seismic ambient noise seismology. _Seismological Research Letter_. 91, 1853-1866.`\n",
    "\n",
    "\n",
    "\n",
    "Chengxin Jiang & Marine Denolle\n",
    "\n",
    "Department of Earth and Planetary Science\n",
    "\n",
    "Harvard University\n",
    "\n",
    "November 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building env for NoisePy\n",
    "\n",
    "Before running this notebook, make sure that you have created and activated the conda env made for NoisePy. If not, you can create one using command lines below (note that jupyter is installed with the command lines here in order to run this notebook). \n",
    "\n",
    "```python\n",
    "conda create -n noisepy -c conda-forge python=3.7 numpy=1.16.2 numba pandas pycwt jupyter mpi4py=3.0.1 obspy=1.1 pyasdf\n",
    "conda activate noisepy\n",
    "git clone https://github.com/noisepy/NoisePy.git\n",
    "```\n",
    "\n",
    "Then you need to activate this notebook with the newly built NoisePy env by invoking the jupyter with the following command line.\n",
    "\n",
    "```python\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Now we can begin to load the modules needed for downloading part first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import obspy\n",
    "import pyasdf\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import UTCDateTime\n",
    "import matplotlib.pyplot as plt\n",
    "from noisepy.seis.datatypes import CCMethod, FreqNorm, RmResp, TimeNorm\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "\n",
    "sys.path.insert(1,'../src')\n",
    "import noise_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Settings for data request\n",
    "\n",
    "Here we download one day of continous data (Z component) for 2 TA stations located west of the Boston city, and we do not remove instrument response (set `rm_resp` to `inv` if you want to use the inventory for instrumental removal), downsample it to 1 Hz and filter it to 0.05-0.4 Hz in the pre-processing. Note that in NoisePy, the way of specifying network, station and channel are slightly different and here just show one example. NoisePy also accepts using a compliated station list for data downloading in S1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download parameters\n",
    "client    = Client('IRIS')                                      # client/data center. see https://docs.obspy.org/packages/obspy.clients.fdsn.html for a list\n",
    "samp_freq = 1                                                   # targeted sampling rate at X samples per seconds \n",
    "rm_resp   = RmResp.NO                                           # select 'no' to not remove response and use 'inv','spectrum','RESP', or 'polozeros' to remove response\n",
    "respdir   = './'                                                # directory where resp files are located (required if rm_resp is neither 'no' nor 'inv')\n",
    "freqmin   = 0.05                                                # pre filtering frequency bandwidth\n",
    "freqmax   = 0.4                                                 # note this cannot exceed Nquist freq                         \n",
    "\n",
    "chan = ['BHZ','BHZ']                                            # channel for each station\n",
    "net  = ['TA','TA']                                              # network for each station \n",
    "sta  = ['K62A','K63A']                                          # station (using a station list is way either compared to specifying stations one by one)\n",
    "start_date = [\"2014_01_01_0_0_0\"]                               # start date of download\n",
    "end_date   = [\"2014_01_02_0_0_0\"]                               # end date of download\n",
    "inc_hours  = 24                                                 # length of data for each request (in hour)\n",
    "nsta       = len(sta)\n",
    "\n",
    "# save prepro parameters into a dic\n",
    "prepro_para = {'rm_resp':rm_resp,'respdir':respdir,'freqmin':freqmin,'freqmax':freqmax,'samp_freq':samp_freq,'start_date':\\\n",
    "    start_date,'end_date':end_date,'inc_hours':inc_hours}\n",
    "\n",
    "# convert time info to UTC format\n",
    "starttime = obspy.UTCDateTime(start_date[0])       \n",
    "endtime   = obspy.UTCDateTime(end_date[0])\n",
    "\n",
    "# another format of time info needed for get_station and get_waveform\n",
    "s1,s2 = noise_module.get_event_list(start_date[0],end_date[0],inc_hours)\n",
    "date_info = {'starttime':starttime,'endtime':endtime} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data and save into ASDF file\n",
    "\n",
    "Note that this step might take a few seconds for data download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write into ASDF file: using start and end time as file name\n",
    "ff=os.path.join('./',s1+'T'+s2+'.h5')\n",
    "with pyasdf.ASDFDataSet(ff,mpi=False,compression=\"gzip-3\",mode='w') as ds:\n",
    "\n",
    "    # loop through each station\n",
    "    for ista in range(nsta):\n",
    "\n",
    "        # get inventory for each station\n",
    "        try:\n",
    "            sta_inv = client.get_stations(network=net[ista],station=sta[ista],\\\n",
    "                location='*',starttime=s1,endtime=s2,level=\"response\")\n",
    "        except Exception as e:\n",
    "            print(e);continue\n",
    "\n",
    "        # add the inventory into ASDF        \n",
    "        try:\n",
    "            ds.add_stationxml(sta_inv) \n",
    "        except Exception: \n",
    "            pass   \n",
    "\n",
    "        try:\n",
    "            # get data\n",
    "            tr = client.get_waveforms(network=net[ista],station=sta[ista],\\\n",
    "                channel=chan[ista],location='*',starttime=s1,endtime=s2)\n",
    "        except Exception as e:\n",
    "            print(e,'for',sta[ista]);continue\n",
    "            \n",
    "        # preprocess to clean data  \n",
    "        print('working on station '+sta[ista])\n",
    "        tr = noise_module.preprocess_raw(tr,sta_inv,prepro_para,date_info)\n",
    "\n",
    "        if len(tr):\n",
    "            new_tags = '{0:s}_00'.format(chan[ista].lower())\n",
    "            ds.add_waveforms(tr,tag=new_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Setup basic parameters for data-processing and cross correlation\n",
    "\n",
    "Next we setup the parameters used for processing the noise data and for later cross correlation computation. As you can find from section below, there are many parameters needed for the computation, which are associated with the input data, options for different processing procedures, cross correlation methods and some tuning parameters. Brief descriptions of each parameter are followed by the variable assignment, but note that more details on this can be found in documentations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfile = glob.glob('./*h5')      # find sac files\n",
    "if not len(sfile):\n",
    "    raise ValueError('Abort! At least 2 sac files are needed!')\n",
    "outpath = './'                     # output dir\n",
    "\n",
    "# parameters of fft_cc\n",
    "cc_len    = 1800                    # window length (sec) to cut daily data into small segments\n",
    "step      = 450                     # overlapping (sec) between the sliding window\n",
    "smooth_N  = 10                      # number of points to be smoothed for running-mean average (time-domain)\n",
    "dt        = 1/samp_freq             # sampling time intervals of the data: in real case it reads from data directly\n",
    "inc_hours = 24                      # basic length (hour) of the continous noise data        \n",
    "\n",
    "freq_norm   = FreqNorm.RMA          # rma-> running mean average for frequency-domain normalization\n",
    "time_norm   = TimeNorm.NO                  # no-> no time-domain normalization; other options are 'rma' for running-mean and 'one-bit'\n",
    "cc_method   = CCMethod.XCORR              # xcorr-> pure cross correlation; other option is 'decon'\n",
    "substack       = False              # sub-stack daily cross-correlation or not\n",
    "substack_len   = cc_len             # how long to stack over: need to be multiples of cc_len\n",
    "smoothspect_N  = 10                 # number of points to be smoothed for running-mean average (freq-domain)\n",
    "\n",
    "# cross-correlation parameters\n",
    "maxlag       = 200                  # time lag (sec) for the cross correlation functions\n",
    "max_over_std = 10                   # amplitude therahold to remove segments of spurious phases \n",
    "\n",
    "# group parameters into a dict\n",
    "fc_para={'samp_freq':samp_freq,'dt':dt,'cc_len':cc_len,'step':step,'freq_norm':freq_norm,'time_norm':time_norm,\\\n",
    "    'cc_method':cc_method,'maxlag':maxlag,'max_over_std':max_over_std,'inc_hours':inc_hours,'smooth_N':smooth_N,\\\n",
    "    'freqmin':freqmin,'freqmax':freqmax,'smoothspect_N':smoothspect_N,'substack':substack,\\\n",
    "    'substack_len':substack_len}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load source data\n",
    "\n",
    "We take the first station in the ASDF file as the source and the other as the receiver. The raw noise data is stored in the $waveform$ structure of the ASDF file and it uses $network.station$ and $channel\\_location$ as the two tags for each station. You would notice that most of the following steps are similar to the ones in the other notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read source and some meta info\n",
    "with pyasdf.ASDFDataSet(sfile[0],mode='r') as ds:\n",
    "    \n",
    "    # station list\n",
    "    sta_list = ds.waveforms.list()\n",
    "    print('source of %s and receiver %s'%(sta_list[0],sta_list[1]))\n",
    "    \n",
    "    # channels for each station\n",
    "    all_tags = ds.waveforms[sta_list[0]].get_waveform_tags()\n",
    "    \n",
    "    # get the source trace\n",
    "    tr_source = ds.waveforms[sta_list[0]][all_tags[0]]\n",
    "    \n",
    "    # read inventory\n",
    "    inv1 = ds.waveforms[sta_list[0]]['StationXML']\n",
    "    \n",
    "    # read station info from inventory\n",
    "    ssta,snet,slon,slat,elv,loc = noise_module.sta_info_from_inv(inv1)\n",
    "\n",
    "# cut source traces into small segments and make statistics\n",
    "trace_stdS,dataS_t,dataS = noise_module.cut_trace_make_stat(fc_para,tr_source)\n",
    "\n",
    "# do fft to freq-domain\n",
    "source_white = noise_module.noise_processing(fc_para,dataS)\n",
    "source_white = np.conjugate(source_white)\n",
    "\n",
    "# num of frequency data\n",
    "Nfft = source_white.shape[1];Nfft2 = Nfft//2\n",
    "\n",
    "# find the right index of good signals\n",
    "sou_ind = np.where((trace_stdS<max_over_std)&(trace_stdS>0)&(np.isnan(trace_stdS)==0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load receiver data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read receiver and some meta info\n",
    "with pyasdf.ASDFDataSet(sfile[0],mode='r') as ds:\n",
    "    \n",
    "    # channels for each station\n",
    "    all_tags = ds.waveforms[sta_list[1]].get_waveform_tags()\n",
    "    \n",
    "    # get the source trace\n",
    "    tr_receiver = ds.waveforms[sta_list[1]][all_tags[0]]\n",
    "    \n",
    "    # read inventory\n",
    "    inv1 = ds.waveforms[sta_list[1]]['StationXML']\n",
    "    \n",
    "    # read station info from inventory\n",
    "    rsta,rnet,rlon,rlat,elv,loc = noise_module.sta_info_from_inv(inv1)\n",
    "\n",
    "# work out distance between source and receiver\n",
    "dist,azi,baz = obspy.geodetics.base.gps2dist_azimuth(slat,slon,rlat,rlon)\n",
    "\n",
    "# cut source traces into small segments and make statistics\n",
    "trace_stdR,dataR_t,dataR = noise_module.cut_trace_make_stat(fc_para,tr_receiver)\n",
    "\n",
    "# do fft to freq-domain\n",
    "receiver_white = noise_module.noise_processing(fc_para,dataR)\n",
    "\n",
    "# find the right index of good signals\n",
    "rec_ind = np.where((trace_stdR<max_over_std)&(trace_stdR>0)&(np.isnan(trace_stdR)==0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the segments of good data for both source and receiver\n",
    "bb=np.intersect1d(sou_ind,rec_ind)\n",
    "if len(bb)==0:raise ValueError('Abort! no good data in overlap')\n",
    "\n",
    "# do cross correlation\n",
    "corr_day,t_corr,n_corr = noise_module.correlate(source_white[bb,:Nfft2],receiver_white[bb,:Nfft2],fc_para,Nfft,dataS_t)\n",
    "\n",
    "# plot the waveform\n",
    "tvec = np.arange(-maxlag,maxlag+dt,dt)\n",
    "plt.figure()\n",
    "plt.plot(tvec,corr_day)\n",
    "plt.xlabel('time [s]')\n",
    "plt.title('cross correlation function between %s and %s'%(ssta,rsta))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save cross correlation data into ASDF file\n",
    "\n",
    "Though we only have one station pair, we can try to save it into the ASDF file. We save the cross correlation data into the auxiliary structure of ASDF, which has two dimentions (data_type and path). In this example, we use the station and network name of the source and receiver station to define the $data\\_type$ and use the channel names to define the $path$. The two tags are chose because the two-dimention variable are enough to define any cross component of the cross correlation functions for any station pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_h5 = 'cc_example_TA.h5'         \n",
    "with pyasdf.ASDFDataSet(cc_h5,mpi=False,mode='w') as ccf_ds:\n",
    "    # location info \n",
    "    coor = {'lonS':slon,'latS':slat,'lonR':rlon,'latR':rlat}\n",
    "    # cross component\n",
    "    comp = tr_source[0].stats.channel[-1]+tr_receiver[0].stats.channel[-1]\n",
    "    # parameters to be saved into ASDF\n",
    "    parameters = noise_module.cc_parameters(fc_para,coor,t_corr,n_corr,comp)\n",
    "\n",
    "    # data_type name as source-receiver pair\n",
    "    data_type = tr_source[0].stats.network+'.'+tr_source[0].stats.station+'_'+tr_receiver[0].stats.network+'.'+tr_receiver[0].stats.station\n",
    "    # path name as cross component\n",
    "    path = comp\n",
    "    # command to save data and parameters into asdf structure\n",
    "    ccf_ds.add_auxiliary_data(data=corr_day, data_type=data_type, path=path, parameters=parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Read the ASDF file\n",
    "\n",
    "Finally, we want to read the cross correlation function we just saved. To retrive the data, we simply need the two tags we just created for the auxiliary structure in ASDF, which are $data\\_type$ and $path$. Note that we do not necessarily need to know the two parameters beforehand, because we can simply get the two parameters from reading the file. You will see how we do it from the codes below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyasdf.ASDFDataSet(cc_h5,mode='r') as ds:\n",
    "    data_type = ds.auxiliary_data.list()\n",
    "    path = ds.auxiliary_data[data_type[0]].list()\n",
    "    print(data_type,path)\n",
    "    \n",
    "    data = ds.auxiliary_data[data_type[0]][path[0]].data[:]\n",
    "    para = ds.auxiliary_data[data_type[0]][path[0]].parameters\n",
    "    \n",
    "    # plot the waveform again\n",
    "    plt.plot(tvec,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end.\n",
    "\n",
    "We hope you enjoy it! Most of the core steps of NoisePy are included here for illustration, and NoisePy is simply adding more loops for source and receiver stations and use some tricks (including embedding parallel functionality) to speed up the general performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
