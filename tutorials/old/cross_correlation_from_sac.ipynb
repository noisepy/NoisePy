{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cross correlation functions from raw noise data in SAC\n",
    "\n",
    "This notebook demontrates the original functionalities of Noisepy from Jiang and Denolle (2019). The core functions are identical to today's version of Noisepy. The difference is that this notebook does not use DataStores for data management and reads directly from ASDF and SAC.\n",
    "\n",
    "It was created by Chengxin Jiang (Chengxin.Jiang1@anu.edu.au) in 2020 and recently updated by Marine Denolle (mdenolle@uw.edu) in 2023.\n",
    "\n",
    "In this notebook, we show some of the key steps in NoisePy for computing cross correlation functions in order for you better understand the unerlying processes of NoisePy script of S1. The input data are daily noise data recorded by two stations and here we show examples in SAC format. \n",
    "\n",
    "The steps to compuate cross correlation functions are:    \n",
    "* Load SAC data and the header info into memory\n",
    "* Break the continous data into small segments with overlaps\n",
    "* Perform Fourier Transform to convert signals into frequency-domain\n",
    "* Calculate cross correlation functions between the small time segments and choose to stack (substack) the cross correlation function of each segment and return to time domain\n",
    "* Save the function in an ASDF file\n",
    "\n",
    "More details on the descriptions of data processing, parameters for different cross correlation method and performance of NoisePy can be found in the online [documentations](https://noise-python.readthedocs.io/en/latest/) and our paper.\n",
    "\n",
    "`Jiang, C. and Denolle, M. 2020. NoisePy: a new high-performance python tool for seismic ambient noise seismology. _Seismological Research Letter_. 91, 1853-1866`\n",
    "\n",
    "\n",
    "\n",
    "Chengxin Jiang & Marine Denolle\n",
    "\n",
    "Department of Earth and Planetary Science\n",
    "\n",
    "Harvard University\n",
    "\n",
    "November 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building env for NoisePy\n",
    "\n",
    "Before running this notebook, make sure that you have created and activated the conda env made for NoisePy. If not, you can create one using command lines below (note that jupyter is installed with the command lines here in order to run this notebook). \n",
    "\n",
    "```python\n",
    "conda create -n noisepy -c conda-forge python=3.7 numpy=1.16.2 numba pandas pycwt jupyter mpi4py=3.0.1 obspy=1.1 pyasdf\n",
    "conda activate noisepy\n",
    "```\n",
    "\n",
    "Then you need to activate this notebook with the newly built NoisePy env by invoking the jupyter with the following command line.\n",
    "\n",
    "```python\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Now we can begin to load the modules needed for this practise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import obspy\n",
    "import scipy\n",
    "import pyasdf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "sys.path.insert(1,'../src')\n",
    "from noisepy.seis.datatypes import CCMethod, FreqNorm, TimeNorm\n",
    "from noisepy.seis import noise_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup basic parameters\n",
    "\n",
    "The fundamental step is to setup the parameters used for cross correlation. As you can find from section below, there are many parameters needed for the computation, which are associated with the input data, some details controlling the targeted processing procedures and some tuning parameters. Some brief descriptions of the parameters are included here following the definination, but note that more details on this can be found in documentations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfiles = glob.glob('../example_data/*.sac')      # find sac files\n",
    "if len(sfiles)<2:\n",
    "    raise ValueError('Abort! At least 2 sac files are needed!')\n",
    "outpath = './'                     # output dir\n",
    "\n",
    "# parameters of fft_cc\n",
    "cc_len    = 1800                    # window length (sec) to cut daily data into small segments\n",
    "step      = 450                     # overlapping (sec) between the sliding window\n",
    "smooth_N  = 10                      # number of points to be smoothed for running-mean average (time-domain)\n",
    "dt        = 0.05                    # sampling time intervals of the data: in real case it reads from data directly\n",
    "sampling_rate = int(1/dt)           # sampling rate \n",
    "inc_hours = 24                      # basic length (hour) of the continous noise data        \n",
    "freqmin   = 0.1                     # frequency range\n",
    "freqmax   = 8            \n",
    "\n",
    "freq_norm   = FreqNorm.RMA          # rma-> running mean average for frequency-domain normalization\n",
    "time_norm   = TimeNorm.NO           # no-> no time-domain normalization; other options are 'rma' for running-mean and 'one-bit'\n",
    "cc_method   = CCMethod.XCORR               # xcorr-> pure cross correlation; other option is 'decon'\n",
    "substack       = False              # sub-stack daily cross-correlation or not\n",
    "substack_len   = cc_len             # how long to stack over: need to be multiples of cc_len\n",
    "smoothspect_N  = 10                 # number of points to be smoothed for running-mean average (freq-domain)\n",
    "\n",
    "# cross-correlation parameters\n",
    "maxlag       = 100                  # time lag (sec) for the cross correlation functions\n",
    "max_over_std = 10                   # amplitude therahold to remove segments of spurious phases \n",
    "\n",
    "# group parameters into a dict\n",
    "fc_para={'sampling_rate':sampling_rate,'dt':dt,'cc_len':cc_len,'step':step,'freq_norm':freq_norm,'time_norm':time_norm,\\\n",
    "    'cc_method':cc_method,'maxlag':maxlag,'max_over_std':max_over_std,'inc_hours':inc_hours,'smooth_N':smooth_N,\\\n",
    "    'freqmin':freqmin,'freqmax':freqmax,'smoothspect_N':smoothspect_N,'substack':substack,\\\n",
    "    'substack_len':substack_len}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read source and some meta info\n",
    "tr_source = obspy.read(sfiles[0])\n",
    "slon = tr_source[0].stats.sac['stlo']\n",
    "slat = tr_source[0].stats.sac['stla']\n",
    "\n",
    "# cut source traces into small segments and make statistics\n",
    "trace_stdS,dataS_t,dataS = noise_module.cut_trace_make_stat(fc_para,tr_source)\n",
    "# do fft to freq-domain\n",
    "source_white = noise_module.noise_processing(fc_para,dataS)\n",
    "source_white = np.conjugate(source_white)\n",
    "# num of frequency data\n",
    "Nfft = source_white.shape[1];Nfft2 = Nfft//2\n",
    "\n",
    "# find the right index of good signals\n",
    "sou_ind = np.where((trace_stdS<max_over_std)&(trace_stdS>0)&(np.isnan(trace_stdS)==0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load receiver data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read receiver and some meta info\n",
    "tr_receiver = obspy.read(sfiles[1])\n",
    "rlon = tr_receiver[0].stats.sac['stlo']\n",
    "rlat = tr_receiver[0].stats.sac['stla']\n",
    "\n",
    "# work out distance between source and receiver\n",
    "dist,azi,baz = obspy.geodetics.base.gps2dist_azimuth(slat,slon,rlat,rlon)\n",
    "\n",
    "# cut source traces into small segments and make statistics\n",
    "trace_stdR,dataR_t,dataR = noise_module.cut_trace_make_stat(fc_para,tr_receiver)\n",
    "\n",
    "# do fft to freq-domain\n",
    "receiver_white = noise_module.noise_processing(fc_para,dataR)\n",
    "\n",
    "# find the right index of good signals\n",
    "rec_ind = np.where((trace_stdR<max_over_std)&(trace_stdR>0)&(np.isnan(trace_stdR)==0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the segments of good data for both source and receiver\n",
    "bb=np.intersect1d(sou_ind,rec_ind)\n",
    "if len(bb)==0:raise ValueError('Abort! no good data in overlap')\n",
    "\n",
    "# do cross correlation\n",
    "corr_day,t_corr,n_corr = noise_module.correlate(source_white[bb,:Nfft2],receiver_white[bb,:Nfft2],fc_para,Nfft,dataS_t)\n",
    "\n",
    "# plot the waveform\n",
    "print(len(corr_day))\n",
    "tvec = np.arange(-maxlag,maxlag+dt,dt)\n",
    "plt.figure()\n",
    "plt.plot(tvec,corr_day)\n",
    "plt.xlabel('time [s]')\n",
    "plt.title('cross correlation function between AYHM and ENZM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save cross correlation data into ASDF file\n",
    "\n",
    "Though we only have one station pair, we can try to save it into the ASDF file. We save the cross correlation data into the auxiliary structure of ASDF, which has two dimentions (data_type and path). In this example, we use the station and network name of the source and receiver station to define the $data\\_type$ and use the channel names to define the $path$. The two tags are chose because the two-dimention variable are enough to define any cross component of the cross correlation functions for any station pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_h5 = 'cc_example.h5'         \n",
    "with pyasdf.ASDFDataSet(cc_h5,mpi=False,mode='w') as ccf_ds:\n",
    "    # location info \n",
    "    coor = {'lonS':slon,'latS':slat,'lonR':rlon,'latR':rlat}\n",
    "    # cross component\n",
    "    comp = tr_source[0].stats.channel[-1]+tr_receiver[0].stats.channel[-1]\n",
    "    # parameters to be saved into ASDF\n",
    "    parameters = noise_module.cc_parameters(fc_para,coor,t_corr,n_corr,comp)\n",
    "\n",
    "    # data_type name as source-receiver pair\n",
    "    data_type = tr_source[0].stats.network+'.'+tr_source[0].stats.station+'_'+tr_receiver[0].stats.network+'.'+tr_receiver[0].stats.station\n",
    "    # path name as cross component\n",
    "    path = comp\n",
    "    # command to save data and parameters into asdf structure\n",
    "    ccf_ds.add_auxiliary_data(data=corr_day, data_type=data_type, path=path, parameters=parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Read the ASDF file\n",
    "\n",
    "Finally, we want to read the cross correlation function we just saved. To retrive the data, we simply need the two tags we just created for the auxiliary structure in ASDF, which are $data\\_type$ and $path$. Note that we do not necessarily need to know the two parameters beforehand, because we can simply get the two parameters from reading the file. You will see how we do it from the codes below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyasdf.ASDFDataSet(cc_h5,mode='r') as ds:\n",
    "    data_type = ds.auxiliary_data.list()\n",
    "    path = ds.auxiliary_data[data_type[0]].list()\n",
    "    print(data_type,path)\n",
    "    \n",
    "    data = ds.auxiliary_data[data_type[0]][path[0]].data[:]\n",
    "    para = ds.auxiliary_data[data_type[0]][path[0]].parameters\n",
    "    print(data,para)\n",
    "    \n",
    "    # plot the waveform again\n",
    "    plt.plot(tvec,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end.\n",
    "\n",
    "Please check the notebook of download_toASDF_cross_correlation.ipynb for how to start from noise data that are stored in ASDF file that is created using S0 of NoisePy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
