{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b0af4c",
   "metadata": {},
   "source": [
    "# NoisePy tutorial: AWS Batch\n",
    "\n",
    "Here's a tutorial on using Amazon EC2 Batch with Fargate Spot and containers to perform a job that involves writing to and reading from AWS S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b78dd",
   "metadata": {},
   "source": [
    "## 1. Checklist and prerequisites\n",
    "\n",
    "### 1.1 Tools\n",
    "You are not required to run this on a AWS EC2 instance, but two tools are required for this tutorail: AWS Command Line Tool (CLI) and JQ. Note that the code cell below only works for x86_64 CentOS where you have sudo permission. You can find installation instructions for other OS below.\n",
    "\n",
    "* AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    "* jq: https://jqlang.github.io/jq/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1287aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AWS CLI (Command line interface)\n",
    "# This tool may already be installed if you are on a EC2 instance running Amazon Linux\n",
    "\n",
    "! curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "! unzip awscliv2.zip\n",
    "! sudo ./aws/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may check the correct installation of CLI with the following command, \n",
    "# which lists the files in SCEDC public bucket.\n",
    "\n",
    "! aws s3 ls s3://scedc-pds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cd99b-bad3-4003-9f60-1e04fa6dce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install jq\n",
    "\n",
    "! sudo yum install -y jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d9595",
   "metadata": {},
   "source": [
    "### 1.2 AWS Account\n",
    "\n",
    "The account ID is a 12-digit number uniquely identify your account. You can find it on your AWS web console.\n",
    "\n",
    "⚠️ Save the workshop `<ACCOUNT_ID>` here: `REPLACE_ME`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10479daa",
   "metadata": {},
   "source": [
    "### 1.3 Role\n",
    "\n",
    "AWS role is a virtual identity that has specific permissions where its ID (called `ARN`) is in the format of `arn:aws:iam::<ACCOUNT_ID>:role/<ROLE>`. AWS batch requires a role to be created for running the jobs. This can be done from the IAM panel on the AWS web console. Depending on the type of service to use, separate roles may be created. A specific role is required for **AWS Batch Service**.\n",
    "- Trusted Entity Type: AWS Service\n",
    "- Use Case: Elastic Container Service\n",
    "    - Elastic Container Service Task\n",
    "- Permission Policies, search and add:\n",
    "    - AmazonECSTaskExecutionRolePolicy\n",
    "    - AmazonS3FullAccess\n",
    "\n",
    "Once the role is created, one more permission is needed:\n",
    "- Go to: Permissions tab --> Add Permissions --> Create inline policy\n",
    "- Search for \"batch\"\n",
    "- Click on **Batch**\n",
    "- Select Read / Describe Jobs\n",
    "- Click Next\n",
    "- Add a policy name, e.g. \"Describe_Batch_Jobs\"\n",
    "- Click Create Policy\n",
    "\n",
    "⚠️ Workshop participants please use `arn:aws:iam::<ACCOUNT_ID>:role/NoisePyBatchRole`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba5c9f",
   "metadata": {},
   "source": [
    "### 1.4 S3 Storage\n",
    "\n",
    "NoisePy uses S3 cloud store to store the cross correlations and stacked data. For this step, it is important that your **role** and the **bucket** have the appropriate permissions for users to read/write into the bucket.\n",
    "\n",
    "The following statement in the JSON format is called a **policy**. It explicitly defined which operation is allowed/denied by which user/role. The following bucket policy defines that \n",
    "* all operations (`\"s3:*\"`) are allowed by your account with attached role (`\"arn:aws:iam::<ACCOUNT_ID>:role/<ROLE>\"`) on any file in the bucket (`\"arn:aws:s3:::<S3_BUCKET>/*\"`).\n",
    "* anyone is allowed to read the data within the bucket (`\"s3:GetObject\"`,`\"s3:GetObjectVersion\"`)\n",
    "* anyone is allowed to list the file within the bucket (`\"s3:ListBucket\"`)\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"Policy1674832359797\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"Stmt1674832357905\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"AWS\": \"arn:aws:iam::<ACCOUNT_ID>:role/<ROLE>\"\n",
    "            },\n",
    "            \"Action\": \"s3:*\",\n",
    "            \"Resource\": \"arn:aws:s3:::<S3_BUCKET>/*\"\n",
    "        },\n",
    "        {\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Principal\": {\n",
    "\t\t\t\t\"AWS\": \"*\"\n",
    "\t\t\t},\n",
    "\t\t\t\"Action\": [\n",
    "\t\t\t\t\"s3:GetObject\",\n",
    "\t\t\t\t\"s3:GetObjectVersion\"\n",
    "\t\t\t],\n",
    "\t\t\t\"Resource\": \"arn:aws:s3:::<S3_BUCKET>/*\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Principal\": {\n",
    "\t\t\t\t\"AWS\": \"*\"\n",
    "\t\t\t},\n",
    "\t\t\t\"Action\": \"s3:ListBucket\",\n",
    "\t\t\t\"Resource\": \"arn:aws:s3:::<S3_BUCKET>\"\n",
    "\t\t}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "⚠️ Save your `<S3_BUCKET>` name here: `REPLACE_ME`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0745af",
   "metadata": {},
   "source": [
    "## 2. Setup Batch Jobs\n",
    "\n",
    "### 2.1 Compute Environment\n",
    "You'll need two pieces of information to create the compute environment. The list of subnets in your VPC and the default security group ID. You can use the following commands to retrieve them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd41df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws ec2 describe-subnets  | jq \".Subnets[] | .SubnetId\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab280446",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws ec2 describe-security-groups --filters \"Name=group-name,Values=default\" | jq \".SecurityGroups[0].GroupId\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e126e65",
   "metadata": {},
   "source": [
    "Use these values to update the missing fields `subnets` and `securityGroupIds` in [compute_environment.yaml](./compute_environment.yaml) and run the code afterwards. If you have multiple subnets, choose one of them.\n",
    "\n",
    "For HPS-book reader, the file is also available [here](https://github.com/noisepy/NoisePy/blob/main/tutorials/cloud/compute_environment.yaml) on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af330592",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws batch create-compute-environment --no-cli-pager --cli-input-yaml file://compute_environment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68826bd9",
   "metadata": {},
   "source": [
    "### 2.2 Create a Job Queue\n",
    "Add the `computeEnvironment` and the `jobQueueName` in [job_queue.yaml](./job_queue.yaml) and then run the following command. \n",
    "\n",
    "For HPS-book reader, the file is also available [here](https://github.com/noisepy/NoisePy/blob/main/tutorials/cloud/job_queue.yaml) on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws batch create-job-queue --no-cli-pager --cli-input-yaml file://job_queue.yaml  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750529a",
   "metadata": {},
   "source": [
    "### 2.3 Create a Job Definition\n",
    "Update the `jobRoleArn` and `executionRoleArn` fields in the [job_definition.yaml](./job_definition.yaml) file with the ARN of the role created in the first step (they should be the same in this case). Add a name for the `jobDefinition` and run the code below.\n",
    "\n",
    "For HPS-book reader, the file is also available [here](https://github.com/noisepy/NoisePy/blob/main/tutorials/cloud/job_definition.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a019f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws batch register-job-definition --no-cli-pager --cli-input-yaml file://job_definition.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb7db6",
   "metadata": {},
   "source": [
    "## 3. Submit the Job\n",
    "### 3.1 Cross-correlation Configuration\n",
    "Update [config.yaml](./config.yaml) for NoisePy configuration. Then copy the file to S3 so that the batch job can access it after launching. Replace the `<S3_BUCKET>` with the bucket we just used, as well as an intermediate `<PATH>` to separate your runs from others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcff491",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp ./config.yaml s3://<S3_BUCKET>/<PATH>/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdbf2d",
   "metadata": {},
   "source": [
    "### 3.2 Run Cross-correlation\n",
    "Update [job_cc.yaml](./job_cc.yaml) with the names of your `jobQueue` and `jobDefinition` created in the last steps. Also give your job a name in `jobName`. Then update the S3 bucket paths to the locations you want to use for the output and your `config.yaml` file.\n",
    "\n",
    "For HPS-book reader, the file is also available [here](https://github.com/noisepy/NoisePy/blob/main/tutorials/cloud/job_cc.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws batch submit-job --no-cli-pager --cli-input-yaml file://job_cc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427c920",
   "metadata": {},
   "source": [
    "### 3.3 Run Stacking\n",
    "Update [job_stack.yaml](./job_stack.yaml) with the names of your `jobQueue` and `jobDefinition` created in the last steps.  Also give your job a name in `jobName`. Then update the S3 bucket paths to the locations you want to use for your input CCFs (e.g. the output of the previous CC run), and the stack output. By default, NoisePy will look for a config file in the `--ccf_path` location to use the same configuration for stacking that was used for cross-correlation.\n",
    "\n",
    "For HPS-book reader, the file is also available [here](https://github.com/noisepy/NoisePy/blob/main/tutorials/cloud/job_stack.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws batch submit-job --no-cli-pager --cli-input-yaml file://job_stack.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a053a1",
   "metadata": {},
   "source": [
    "### 4. Visualization\n",
    "You can use plot_stacks tutorials for cross-correlation visualization after all jobs return. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
